{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Data Access\n",
    "\n",
    "**Author:** Alexander Bates  \n",
    "**Python Version**\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial covers data access to a pre-prepared curation of connectome data for the major *Drosophila* connectome projects. Our curated data includes:\n",
    "\n",
    "- **Whole-system datasets**: maleCNS, BANC (brain + ventral nerve cord)\n",
    "- **Brain-only datasets**: FAFB, Hemibrain\n",
    "- **VNC-only dataset**: MANC\n",
    "\n",
    "This Python tutorial demonstrates:\n",
    "- Direct GCS data access using `gcsfs` and `pyarrow`\n",
    "- Efficient streaming of large Parquet files\n",
    "- Interactive visualizations with Plotly\n",
    "- Metadata exploration and filtering\n",
    "- Synapse data analysis with lazy loading\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dataset configuration\n# Options: \"banc_746\", \"fafb_783\", \"manc_121\", \"hemibrain_121\", \"malecns_09\"\nDATASET = \"banc_746\"\nDATASET_ID = \"banc_746_id\"\n\n# Data location - can be GCS bucket or local path\n# Option 1 (GCS - default): Access data directly from Google Cloud Storage\nDATA_PATH = \"gs://brain-and-nerve-cord_exports/sjcabs_data\"\n\n# Option 2 (Local): Use local copy if you've downloaded the data\n# DATA_PATH = \"/path/to/local/sjcabs_data\"\n# Example: DATA_PATH = \"~/data/sjcabs_data\"\n\n# Detect if using GCS or local path\nUSE_GCS = DATA_PATH.startswith(\"gs://\")\n\n# Image output directory\nimport os\nIMG_DIR = \"images/tutorial_01\"\nos.makedirs(IMG_DIR, exist_ok=True)\n\nprint(f\"Dataset: {DATASET}\")\nprint(f\"Data location: {DATA_PATH}\")\nprint(f\"Using GCS: {USE_GCS}\")\nprint(f\"Images will be saved to: {IMG_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "We'll use:\n",
    "- **pandas**: Data manipulation and analysis\n",
    "- **pyarrow**: Efficient reading of Feather and Parquet files\n",
    "- **gcsfs**: Google Cloud Storage filesystem interface\n",
    "- **plotly**: Interactive visualizations\n",
    "- **navis**: Neuron analysis tools (for future tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.feather as feather\n",
    "import pyarrow.parquet as pq\n",
    "import gcsfs\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"âœ“ Packages loaded successfully\")\n",
    "print(f\"Python version: {pd.__version__} (pandas), {pa.__version__} (pyarrow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Location Options\n",
    "\n",
    "This tutorial supports two data access modes:\n",
    "\n",
    "### Option 1: Direct GCS Access (Default)\n",
    "Access data directly from Google Cloud Storage - no manual download required!\n",
    "- **Pros:** No local storage needed, always up-to-date\n",
    "- **Cons:** Slower for large files, requires authentication & internet\n",
    "\n",
    "**GCS bucket location:** `gs://brain-and-nerve-cord_exports/sjcabs_data/`\n",
    "\n",
    "### Option 2: Local Access (Faster for Repeated Use)\n",
    "Download data once with `gsutil`, then access locally:\n",
    "\n",
    "```bash\n",
    "# Download specific dataset (e.g., BANC metadata + synapses)\n",
    "gsutil -m cp gs://brain-and-nerve-cord_exports/sjcabs_data/banc/banc_746_meta.feather ~/data/sjcabs_data/banc/\n",
    "gsutil -m cp gs://brain-and-nerve-cord_exports/sjcabs_data/banc/banc_746_synapses.parquet ~/data/sjcabs_data/banc/\n",
    "\n",
    "# Or download entire dataset directory\n",
    "gsutil -m cp -r gs://brain-and-nerve-cord_exports/sjcabs_data/banc ~/data/sjcabs_data/\n",
    "```\n",
    "\n",
    "Then update the configuration cell:\n",
    "```python\n",
    "DATA_PATH = \"~/data/sjcabs_data\"  # Use your local path\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Setup GCS Access\n",
    "\n",
    "**Authentication required:** Before running this tutorial with GCS, authenticate with Google Cloud:\n",
    "\n",
    "```bash\n",
    "# Install gcloud CLI if you haven't already:\n",
    "# https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "# Authenticate with your Google account\n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "This creates credentials that `gcsfs` will use automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GCS filesystem if needed\n",
    "if USE_GCS:\n",
    "    print(\"Setting up Google Cloud Storage access...\")\n",
    "    gcs = gcsfs.GCSFileSystem(token='google_default')\n",
    "    print(\"âœ“ GCS filesystem initialized\")\n",
    "else:\n",
    "    gcs = None\n",
    "    print(\"Using local filesystem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Define utility functions for path construction and data reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def construct_path(data_root, dataset, file_type=\"meta\", space_suffix=None):\n    \"\"\"\n    Construct file paths for dataset files.\n    Note: Skeleton files do not include version numbers in their filenames.\n    \n    Parameters\n    ----------\n    data_root : str\n        Root data directory (can be gs:// or local path)\n    dataset : str\n        Dataset name with version (e.g., \"banc_746\")\n    file_type : str\n        Type of file: \"meta\", \"synapses\", \"edgelist\", \"skeletons\"\n    space_suffix : str, optional\n        Space name for skeletons (defaults to native space, e.g., \"banc_space\")\n    \n    Returns\n    -------\n    str\n        Full path to the file\n    \"\"\"\n    # Extract dataset name (e.g., \"banc\" from \"banc_746\")\n    dataset_name = dataset.split(\"_\")[0]\n    \n    # Determine file extension\n    extensions = {\n        \"meta\": \".feather\",\n        \"synapses\": \".parquet\",\n        \"edgelist\": \".feather\",\n        \"edgelist_simple\": \".feather\",\n        \"skeletons\": \"\"  # No extension - it's a directory\n    }\n    \n    if file_type not in extensions:\n        raise ValueError(f\"Unknown file_type: {file_type}. Choose from: {list(extensions.keys())}\")\n    \n    extension = extensions[file_type]\n    \n    # Construct filename\n    if file_type == \"skeletons\":\n        # Skeleton files don't include version number and have specific naming\n        # Pattern: {dataset_name}_{space_name}_[l2_]swc (directory)\n        # e.g., banc_banc_space_l2_swc/, fafb_fafb_space_swc/\n        \n        # Default space is the native space for the dataset\n        if space_suffix is None:\n            space_suffix = f\"{dataset_name}_space\"\n        \n        # BANC uses l2 skeletons, others don't\n        if dataset_name == \"banc\":\n            filename = f\"{dataset_name}_{space_suffix}_l2_swc{extension}\"\n        else:\n            filename = f\"{dataset_name}_{space_suffix}_swc{extension}\"\n    else:\n        # Other file types include the full dataset name with version\n        filename = f\"{dataset}_{file_type}{extension}\"\n    \n    # Combine into full path\n    full_path = f\"{data_root}/{dataset_name}/{filename}\"\n    \n    return full_path\n\ndef read_feather_gcs(path, gcs_fs=None):\n    \"\"\"\n    Read Feather file from GCS or local path.\n    \n    Parameters\n    ----------\n    path : str\n        Path to Feather file (can be gs:// or local)\n    gcs_fs : gcsfs.GCSFileSystem, optional\n        GCS filesystem object (required for GCS paths)\n    \n    Returns\n    -------\n    pd.DataFrame\n        Loaded data\n    \"\"\"\n    if path.startswith(\"gs://\"):\n        if gcs_fs is None:\n            raise ValueError(\"gcs_fs required for GCS paths\")\n        \n        print(f\"Reading from GCS: {path}\")\n        \n        # Strip gs:// prefix for gcsfs\n        gcs_path = path.replace(\"gs://\", \"\")\n        \n        with gcs_fs.open(gcs_path, 'rb') as f:\n            df = feather.read_feather(f)\n        \n        print(f\"âœ“ Loaded {len(df):,} rows\")\n        return df\n    else:\n        print(f\"Reading from local path: {path}\")\n        df = pd.read_feather(path)\n        print(f\"âœ“ Loaded {len(df):,} rows\")\n        return df\n\n\ndef save_plot(fig, name, width=1200, height=600):\n    \"\"\"\n    Save plotly figure to image file.\n    \n    Parameters\n    ----------\n    fig : plotly.graph_objects.Figure\n        Plotly figure to save\n    name : str\n        Filename (without extension)\n    width : int\n        Width in pixels\n    height : int\n        Height in pixels\n    \"\"\"\n    filename = os.path.join(IMG_DIR, f\"{name}.png\")\n    fig.write_image(filename, width=width, height=height, scale=2)\n    print(f\"âœ“ Saved plot to {filename}\")\n\n\nprint(\"âœ“ Helper functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup File Paths\n",
    "\n",
    "Construct paths to metadata and synapse files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct file paths\n",
    "meta_path = construct_path(DATA_PATH, DATASET, \"meta\")\n",
    "synapse_path = construct_path(DATA_PATH, DATASET, \"synapses\")\n",
    "\n",
    "print(\"File paths:\")\n",
    "print(f\"  Metadata: {meta_path}\")\n",
    "print(f\"  Synapses: {synapse_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reading Connectome Metadata\n",
    "\n",
    "### Understanding the File Formats\n",
    "\n",
    "Our data files use two Apache Arrow formats:\n",
    "- **Feather** (`.feather`) for metadata - smaller files (~10 MB), loaded entirely into memory\n",
    "- **Parquet** (`.parquet`) for synapses - large files (4-15 GB), supports lazy loading and predicate pushdown\n",
    "\n",
    "**Why Parquet for synapses?**\n",
    "- âœ“ Column selection: download only needed columns\n",
    "- âœ“ Row filtering: filter on the server before downloading\n",
    "- âœ“ Compression: smaller file sizes\n",
    "- âœ“ Efficient for analytical queries on large datasets\n",
    "\n",
    "### Load Metadata\n",
    "\n",
    "For metadata (~10 MB), we can load the entire dataset into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata into memory\n",
    "meta_full = read_feather_gcs(meta_path, gcs_fs=gcs)\n",
    "\n",
    "print(f\"\\nDataset: {DATASET}\")\n",
    "print(f\"Total neurons: {len(meta_full):,}\")\n",
    "print(f\"Metadata columns: {len(meta_full.columns)}\")\n",
    "print(f\"\\nAvailable columns:\")\n",
    "print(meta_full.columns.tolist())\n",
    "\n",
    "# Display first few rows\n",
    "meta_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proofread Neurons (Important Concept!)\n",
    "\n",
    "This meta data table contains all of the \"identified\" neurons in the dataset.\n",
    "\n",
    "You may encounter neuron IDs outside of this meta data table, e.g., in the synapse table. Those are \"fragments\" that have not been linked up to full neurons.\n",
    "\n",
    "Let's get our list of **\"proofread\" identified neurons**, as they are what we will want for analysis, mainly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proofread neuron IDs\n",
    "# These are the validated, manually curated neurons in the dataset\n",
    "proofread_ids = meta_full[DATASET_ID].values\n",
    "\n",
    "print(f\"Number of proofread neurons: {len(proofread_ids):,}\")\n",
    "print(f\"Example IDs: {proofread_ids[:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Filtering Kenyon Cells\n",
    "\n",
    "[Kenyon cells](https://en.wikipedia.org/wiki/Kenyon_cell) are the principal neurons of the insect [mushroom body](https://en.wikipedia.org/wiki/Mushroom_bodies), forming parallel pathways for associative memory. They integrate multi-sensory (but mostly olfactory) information and can number in the thousands per fly brain.\n",
    "\n",
    "Let's filter for Kenyon cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Kenyon cells\n",
    "kenyon_cells = meta_full[meta_full['cell_class'].str.contains('kenyon_cell', case=False, na=False)]\n",
    "\n",
    "print(f\"Found {len(kenyon_cells):,} Kenyon cells in {DATASET}\")\n",
    "kenyon_cells.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploring the Metadata\n",
    "\n",
    "### Hierarchical Classification\n",
    "\n",
    "The metadata uses a hierarchical classification system.\n",
    "\n",
    "**Hierarchy:** flow â†’ super_class â†’ cell_class â†’ cell_sub_class â†’ cell_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full metadata\n",
    "meta = meta_full.copy()\n",
    "\n",
    "# Count neurons by classification level\n",
    "flow_counts = meta['flow'].value_counts().dropna().sort_values(ascending=False)\n",
    "super_counts = meta['super_class'].value_counts().dropna().sort_values(ascending=False)\n",
    "class_counts = meta['cell_class'].value_counts().dropna().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nFlow categories:\")\n",
    "print(flow_counts)\n",
    "\n",
    "print(\"\\nTop 10 super_classes:\")\n",
    "print(super_counts.head(10))\n",
    "\n",
    "print(\"\\nTop 10 cell_classes:\")\n",
    "print(class_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurotransmitter Distribution\n",
    "\n",
    "Neurotransmitter predictions are based on [Eckstein & Bates et al. (2024) *Cell*](https://doi.org/10.1016/j.cell.2024.03.016).\n",
    "\n",
    "Let's visualize the distribution using Plotly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count neurotransmitter predictions\nnt_counts = meta['neurotransmitter_predicted'].value_counts().dropna().sort_values(ascending=True)\n\n# Create interactive bar plot with Plotly\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    y=nt_counts.index,\n    x=nt_counts.values,\n    orientation='h',\n    marker=dict(\n        color=nt_counts.values,\n        colorscale='Viridis',\n        showscale=False\n    ),\n    text=nt_counts.values,\n    textposition='outside',\n    texttemplate='%{text:,}',\n    hovertemplate='<b>%{y}</b><br>Count: %{x:,}<extra></extra>'\n))\n\nfig.update_layout(\n    title=dict(\n        text=f\"Neurotransmitter Predictions: {DATASET}<br><sub>Based on Eckstein & Bates et al. (2024)</sub>\",\n        x=0.5,\n        xanchor='center'\n    ),\n    xaxis_title=\"Number of Neurons\",\n    yaxis_title=\"Predicted Neurotransmitter\",\n    height=400,\n    template=\"plotly_white\",\n    font=dict(size=12),\n    margin=dict(l=150, r=100, t=100, b=50)\n)\n\n# Save plot\ntry:\n    save_plot(fig, \"neurotransmitter_distribution\", width=1000, height=500)\nexcept Exception as e:\n    print(f\"Note: Could not save plot: {e}\")\n\nfig.show()\n\n# Save plot\ntry:\n    save_plot(fig, \"neurotransmitter_distribution\", width=1000, height=500)\nexcept Exception as e:\n    print(f\"Note: Could not save plot (likely missing kaleido package): {e}\")\n    print(\"To enable saving: pip install -U kaleido\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding Neuropils\n\nOur synapses have been roughly mapped to \"neuropils\", which are human-determined regions of the nervous system. These determinations are based on lumps and grooves on the surface of neural tissue and boundaries in synapse densities, but they roughly correlate with functional circuits. At least in some cases.\n\nOur brain neuropils are transformed into connectome spaces from Ito et al., 2014's demarcations at light-level, see [here](https://pubmed.ncbi.nlm.nih.gov/24559671/). See below.\n\n<p align=\"center\">\n  <img src=\"../inst/images/brain_neuropils_ito_et_al_2014.jpg\" alt=\"Brain Neuropils from Ito et al. 2014\" width=\"80%\">\n</p>\n\nOur ventral nerve cord neuropils come from Court et al. 2020, see [here](https://pubmed.ncbi.nlm.nih.gov/32931755/). See below.\n\n<p align=\"center\">\n  <img src=\"../inst/images/vnc_neuropils_court_et_al_2020.jpg\" alt=\"VNC Neuropils from Court et al. 2020\" width=\"80%\">\n</p>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Working with Synapse Data (Parquet + Lazy Loading)\n",
    "\n",
    "Synapse files are large (4-15 GB). With **Parquet format** and PyArrow's capabilities:\n",
    "- âœ“ Filter and select BEFORE loading into RAM\n",
    "- âœ“ Only loads matching rows and selected columns\n",
    "- âœ“ Works on multi-GB files efficiently\n",
    "\n",
    "**For GCS files:** PyArrow can stream and filter directly from cloud storage!\n",
    "\n",
    "### Filtering Mushroom Body Synapses\n",
    "\n",
    "The [mushroom body](https://en.wikipedia.org/wiki/Mushroom_bodies) (MB) is the insect brain structure for associative learning and memory. Let's extract MB synapses using pattern matching.\n",
    "\n",
    "We'll use PyArrow's `ParquetDataset` for efficient filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FILTERING PARQUET DATA ===\")\n",
    "print(f\"\\nReading synapses from: {synapse_path}\")\n",
    "print(\"(Using PyArrow for efficient filtering)\\n\")\n",
    "\n",
    "# Setup filesystem for reading\n",
    "if USE_GCS:\n",
    "    # Strip gs:// prefix for PyArrow\n",
    "    parquet_path = synapse_path.replace(\"gs://\", \"\")\n",
    "    filesystem = gcs\n",
    "else:\n",
    "    parquet_path = synapse_path\n",
    "    filesystem = None\n",
    "\n",
    "print(\"Reading Parquet file (selecting specific columns)...\")\n",
    "print(\"Filtering for right side only to reduce data size...\")\n",
    "print(\"This may take a few minutes for large files...\")\n",
    "\n",
    "# Read only the columns we need\n",
    "# PyArrow will stream and only load these columns\n",
    "table = pq.read_table(\n",
    "    parquet_path,\n",
    "    columns=['id', 'pre', 'post', 'neuropil', 'side'],\n",
    "    filesystem=filesystem\n",
    ")\n",
    "\n",
    "print(\"Converting to pandas and filtering...\")\n",
    "\n",
    "# Convert to pandas\n",
    "synapses_subset = table.to_pandas()\n",
    "\n",
    "# Filter for MB neuropils (right side only) and proofread neurons\n",
    "print(\"Filtering for mushroom body synapses (neuropil starts with 'MB', right side only)...\")\n",
    "print(\"Also filtering for proofread neurons only...\")\n",
    "\n",
    "mb_synapses = synapses_subset[\n",
    "    synapses_subset['neuropil'].str.startswith('MB', na=False) &\n",
    "    (synapses_subset['side'] == 'right') &\n",
    "    (synapses_subset['pre'].isin(proofread_ids) | synapses_subset['post'].isin(proofread_ids))\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nâœ“ Done! Loaded {len(mb_synapses):,} mushroom body synapses (right side only)\")\n",
    "print(f\"Unique presynaptic neurons/fragments: {mb_synapses['pre'].nunique():,}\")\n",
    "print(f\"Unique postsynaptic neurons/fragments: {mb_synapses['post'].nunique():,}\")\n",
    "\n",
    "mb_synapses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Mushroom Body Neurons\n",
    "\n",
    "Define MB neurons as those with â‰¥100 synapses (inputs or outputs) within the MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count outputs per neuron (filter for proofread)\n",
    "mb_outputs = (\n",
    "    mb_synapses[mb_synapses['pre'].isin(proofread_ids)]\n",
    "    .groupby('pre')\n",
    "    .size()\n",
    "    .reset_index(name='n_outputs')\n",
    "    .query('n_outputs >= 100')\n",
    ")\n",
    "\n",
    "# Count inputs per neuron (filter for proofread)\n",
    "mb_inputs = (\n",
    "    mb_synapses[mb_synapses['post'].isin(proofread_ids)]\n",
    "    .groupby('post')\n",
    "    .size()\n",
    "    .reset_index(name='n_inputs')\n",
    "    .query('n_inputs >= 100')\n",
    ")\n",
    "\n",
    "# Combine to get all MB neurons\n",
    "mb_neurons = pd.unique(pd.concat([mb_outputs['pre'], mb_inputs['post']]))\n",
    "print(f\"Neurons with â‰¥100 synapses in MB: {len(mb_neurons):,}\")\n",
    "\n",
    "# Get metadata for MB neurons\n",
    "mb_meta = meta[meta[DATASET_ID].isin(mb_neurons)].copy()\n",
    "\n",
    "# Check how many are Kenyon cells\n",
    "n_kc = mb_meta['cell_class'].str.contains('kenyon_cell', case=False, na=False).sum()\n",
    "n_other = len(mb_meta) - n_kc\n",
    "\n",
    "print(f\"  Kenyon cells: {n_kc:,}\")\n",
    "print(f\"  Other neurons: {n_other:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterize Non-Kenyon MB Neurons\n",
    "\n",
    "What other neuron types are present in the mushroom body?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data - focus on non-Kenyon cells\nmb_meta_clean = mb_meta.copy()\nmb_meta_clean['super_class'] = mb_meta_clean['super_class'].fillna('other')\nmb_meta_clean['cell_class'] = mb_meta_clean['cell_class'].fillna('other')\nmb_meta_clean['cell_sub_class'] = mb_meta_clean['cell_sub_class'].fillna('other')\nmb_meta_clean['is_kenyon'] = mb_meta_clean['cell_class'].str.contains('kenyon_cell', case=False, na=False)\n\n# Filter out Kenyon cells\nmb_non_kc = mb_meta_clean[~mb_meta_clean['is_kenyon']]\n\n# Count by classification levels\nsuper_counts = mb_non_kc['super_class'].value_counts().sort_values(ascending=True)\nclass_counts = mb_non_kc['cell_class'].value_counts().head(15).sort_values(ascending=True)\nsubclass_counts = mb_non_kc['cell_sub_class'].value_counts().head(15).sort_values(ascending=True)\n\n# Create subplots\nfig = make_subplots(\n    rows=3, cols=1,\n    subplot_titles=('Super Class', 'Cell Class (Top 15)', 'Cell Sub-Class (Top 15)'),\n    vertical_spacing=0.12,\n    specs=[[{\"type\": \"bar\"}], [{\"type\": \"bar\"}], [{\"type\": \"bar\"}]]\n)\n\n# Super class\nfig.add_trace(\n    go.Bar(\n        y=super_counts.index,\n        x=super_counts.values,\n        orientation='h',\n        marker_color='lightblue',\n        text=super_counts.values,\n        textposition='outside',\n        hovertemplate='<b>%{y}</b><br>Count: %{x}<extra></extra>'\n    ),\n    row=1, col=1\n)\n\n# Cell class\nfig.add_trace(\n    go.Bar(\n        y=class_counts.index,\n        x=class_counts.values,\n        orientation='h',\n        marker_color='lightgreen',\n        text=class_counts.values,\n        textposition='outside',\n        hovertemplate='<b>%{y}</b><br>Count: %{x}<extra></extra>'\n    ),\n    row=2, col=1\n)\n\n# Cell sub-class\nfig.add_trace(\n    go.Bar(\n        y=subclass_counts.index,\n        x=subclass_counts.values,\n        orientation='h',\n        marker_color='lightcoral',\n        text=subclass_counts.values,\n        textposition='outside',\n        hovertemplate='<b>%{y}</b><br>Count: %{x}<extra></extra>'\n    ),\n    row=3, col=1\n)\n\nfig.update_layout(\n    title_text=f\"Non-Kenyon Mushroom Body Neurons: {DATASET}\",\n    title_x=0.5,\n    title_xanchor='center',\n    height=1200,\n    showlegend=False,\n    template=\"plotly_white\"\n)\n\nfig.update_xaxes(title_text=\"Count\")\n\n# Save plot\ntry:\n    save_plot(fig, \"mb_neuron_classification\", width=1200, height=1200)\nexcept Exception as e:\n    print(f\"Note: Could not save plot: {e}\")\n\nfig.show()\n\n# Save plot\ntry:\n    save_plot(fig, \"mb_neuron_classification\", width=1200, height=1200)\nexcept Exception as e:\n    print(f\"Note: Could not save plot: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Visualization\n",
    "\n",
    "Create a summary comparing Kenyon vs non-Kenyon MB neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare summary data\nmb_summary = pd.DataFrame({\n    'Category': ['Kenyon Cells', 'Other MB Neurons'],\n    'Count': [n_kc, n_other]\n})\n\nmb_summary['Percentage'] = (mb_summary['Count'] / mb_summary['Count'].sum() * 100).round(1)\n\n# Create pie chart\nfig = go.Figure()\n\nfig.add_trace(go.Pie(\n    labels=mb_summary['Category'],\n    values=mb_summary['Count'],\n    text=mb_summary.apply(lambda x: f\"{x['Count']:,}<br>({x['Percentage']:.1f}%)\", axis=1),\n    textposition='inside',\n    textfont=dict(size=14, color='white'),\n    marker=dict(colors=['#E69F00', '#56B4E9']),\n    hovertemplate='<b>%{label}</b><br>Count: %{value:,}<br>Percentage: %{percent}<extra></extra>'\n))\n\nfig.update_layout(\n    title=dict(\n        text=f\"Mushroom Body Neurons: {DATASET}<br><sub>Neurons with â‰¥100 synapses in MB</sub>\",\n        x=0.5,\n        xanchor='center'\n    ),\n    height=500,\n    template=\"plotly_white\"\n)\n\n# Save plot\ntry:\n    save_plot(fig, \"mb_neuron_summary\", width=800, height=500)\nexcept Exception as e:\n    print(f\"Note: Could not save plot: {e}\")\n\nfig.show()\n\n# Save plot\ntry:\n    save_plot(fig, \"mb_neuron_summary\", width=800, height=500)\nexcept Exception as e:\n    print(f\"Note: Could not save plot: {e}\")\n\n# Print summary\nprint(\"\\nMushroom Body Summary:\")\nprint(mb_summary.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Data Access**: Loading connectome data from GCS or local files\n",
    "2. **Metadata Exploration**: Understanding the hierarchical classification system\n",
    "3. **Proofread Neurons**: Important concept for filtering validated neurons\n",
    "4. **Efficient Data Loading**: Using PyArrow and Parquet for large synapse files\n",
    "5. **Data Analysis**: Filtering and characterizing mushroom body neurons\n",
    "6. **Visualization**: Creating interactive plots with Plotly\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Always filter for proofread neurons** when analyzing connectivity\n",
    "- **Use Parquet for large files** - it supports efficient filtering and column selection\n",
    "- **Stream from GCS** when possible to avoid large downloads\n",
    "- **Interactive plots** with Plotly make exploration easier\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Try exploring other datasets or neuropils:\n",
    "- Change `DATASET` to explore other connectomes (fafb_783, manc_121, etc.)\n",
    "- Filter for different neuropils (e.g., 'AL' for antennal lobe, 'LH' for lateral horn)\n",
    "- Analyze different cell types or neurotransmitters\n",
    "- Look at Tutorial 02 for neuron morphology analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Tutorial complete!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}